context:
  name: llama.cpp
  version: "5649"
  build: 0

package:
  name: ${{ name|lower }}
  version: ${{ version }}

source:
  url: https://github.com/ggml-org/${{ name }}/archive/b${{ version | split(".") | list | last }}.tar.gz
  sha256: 4e76e8a320d207bd5bfd75dae77d6e3b81dec950832a07a7e9441e30bcd95d96

build:
  skip:
    # Skip building on Windows with CUDA because compile time is prohibitive.
    - win and cuda_compiler_version != "None"

  number: ${{ build }}
  string: '{%- if cuda_compiler_version != "None" -%}cuda${{ cuda_compiler_version | replace(".", "") }}_h${{ hash }}_${{ build }}{%- elif (osx and x86_64) or cuda_compiler_version == "None" -%}cpu_${{ blas_impl }}_h${{ hash }}_${{ build }}{%- elif osx and arm64 -%}mps_h${{ hash }}_${{ build }}{%- endif -%}'

  script:
    - if: unix
      then: |
        echo hello
        LLAMA_ARGS="-DLLAMA_BUILD_TESTS=OFF"

        {%- macro llama_args(value) %}
        LLAMA_ARGS="${LLAMA_ARGS} -DLLAMA_${{ value }}"
        {%- endmacro %}

        {%- macro cmake_args(value) -%}
        LLAMA_ARGS="${LLAMA_ARGS} -D${{ value }}"
        {%- endmacro %}

        {% macro ggml_args(value) -%}
        LLAMA_ARGS="${LLAMA_ARGS} -DGGML_${{ value }}"
        {%- endmacro %}

        ${{ cmake_args("BUILD_SHARED_LIBS=ON") }}
        ${{ llama_args("CURL=OFF") }}

        {%- if osx and arm64 %}
        ${{ ggml_args("NATIVE=OFF") }}
        ${{ ggml_args("AVX=OFF") }}
        ${{ ggml_args("AVX2=OFF") }}
        ${{ ggml_args("FMA=OFF") }}
        ${{ ggml_args("F16C=OFF") }}
        ${{ ggml_args("METAL=ON") }}
        ${{ ggml_args("ACCELERATE=ON") }}
        {%- endif %}

        {%- if osx and x86_64 %}
        ${{ ggml_args("METAL=OFF") }}
        ${{ ggml_args("ACCELERATE=ON") }}
        {%- endif %}

        {%- if cuda_compiler_version != "None" %}
        ${{ ggml_args("CUDA=ON") }}
        ${{ cmake_args("CMAKE_CUDA_ARCHITECTURES=all") }}
        {%- endif %}

        {%- if not osx and cuda_compiler_version == "None" %}
        ${{ ggml_args("BLAS=ON") }}

        {%- if blas_impl == "mkl" %}
        ${{ ggml_args("BLAS_VENDOR=Intel10_64_dyn") }}
        {%- endif %}

        {%- endif %}

        echo $LLAMA_ARGS
        cmake -S . -B build -G Ninja ${CMAKE_ARGS} ${LLAMA_ARGS}
        cmake --build build
        cmake --install build
    - if: win
      then: |
        echo hello
        set LLAMA_ARGS=-DLLAMA_BUILD_TESTS=OFF

        {% macro llama_args(value) -%}
        set LLAMA_ARGS=%LLAMA_ARGS% -DLLAMA_${{ value }}
        {%- endmacro %}

        {% macro cmake_args(value) -%}
        set LLAMA_ARGS=%LLAMA_ARGS% -D${{ value }}
        {%- endmacro %}

        {% macro ggml_args(value) -%}
        set LLAMA_ARGS=%LLAMA_ARGS% -DGGML_${{ value }}
        {%- endmacro %}

        ${{ cmake_args("BUILD_SHARED_LIBS=ON") }}
        ${{ llama_args("CURL=OFF") }}

        ${{ ggml_args("NATIVE=OFF") }}

        {%- if cuda_compiler_version != "None" %}
        ${{ ggml_args("CUDA=ON") }}
        ${{ cmake_args("CMAKE_CUDA_ARCHITECTURES=all") }}

        {%- else %}

        ${{ ggml_args("BLAS=ON") }}

        {%- if blas_impl == "mkl" %}
        ${{ ggml_args("BLAS_VENDOR=Intel10_64_dyn") }}
        {%- endif %}

        {%- endif %}

        set LLAMA_ARGS
        cmake -S . -B build -G Ninja %CMAKE_ARGS% %LLAMA_ARGS%
        cmake --build build
        cmake --install build
  dynamic_linking:
    missing_dso_allowlist:
      - if: win
        then:
          - "*/nvcuda.dll"
      - if: linux
        then:
          - "*/libcuda.so.1"

requirements:
  build:
    - ${{ compiler('c') }}
    - ${{ stdlib('c') }}
    - ${{ compiler('cxx') }}
    - if: cuda_compiler_version != "None"
      then:
        - ${{ compiler('cuda') }}
    - cmake
    - git
    - ninja
    - pkgconfig
  host:
    - if: cuda_compiler_version != "None"
      then:
        # NOTE: Without cuda-version, we are installing cuda-toolkit 11.8 instead of 11.2!
        - cuda-version ${{ cuda_compiler_version }}.*
        - if: match(cuda_compiler_version, "12.*")
          then:
            - cuda-cudart-dev ${{ cuda_compiler_version }}.*
            - libcublas-dev ${{ cuda_compiler_version }}.*

    - if: (not osx) and (cuda_compiler_version == "None") and (blas_impl == "mkl")
      then:
        - blas-devel * *_${{ blas_impl }}
        - mkl-devel ${{ mkl }}.*
  run:
    - if: cuda_compiler_version != "None"
      then:
        - cuda-version ${{ cuda_compiler_version }}.*
        - __cuda
        - if: match(cuda_compiler_version, "12.*")
          then:
            - cuda-nvcc-tools
  run_constraints:
    # whisper.cpp also vendors ggml
    - whisper.cpp <0.0.0a0

tests:
  # Test for presence of header files, libraries, and configuration files
  - if: unix
    then:
      - script:
          # Check binaries
          - test -f "$PREFIX/bin/convert_hf_to_gguf.py"
          - test -f "$PREFIX/bin/llama-batched"
          - test -f "$PREFIX/bin/llama-batched-bench"
          - test -f "$PREFIX/bin/llama-bench"
          - test -f "$PREFIX/bin/llama-cli"
          - test -f "$PREFIX/bin/llama-convert-llama2c-to-ggml"
          - test -f "$PREFIX/bin/llama-cvector-generator"
          - test -f "$PREFIX/bin/llama-embedding"
          - test -f "$PREFIX/bin/llama-eval-callback"
          - test -f "$PREFIX/bin/llama-export-lora"
          - test -f "$PREFIX/bin/llama-finetune"
          - test -f "$PREFIX/bin/llama-gen-docs"
          - test -f "$PREFIX/bin/llama-gguf"
          - test -f "$PREFIX/bin/llama-gguf-hash"
          - test -f "$PREFIX/bin/llama-gguf-split"
          - test -f "$PREFIX/bin/llama-gritlm"
          - test -f "$PREFIX/bin/llama-imatrix"
          - test -f "$PREFIX/bin/llama-lookahead"
          - test -f "$PREFIX/bin/llama-lookup"
          - test -f "$PREFIX/bin/llama-lookup-create"
          - test -f "$PREFIX/bin/llama-lookup-merge"
          - test -f "$PREFIX/bin/llama-lookup-stats"
          - test -f "$PREFIX/bin/llama-mtmd-cli"
          - test -f "$PREFIX/bin/llama-parallel"
          - test -f "$PREFIX/bin/llama-passkey"
          - test -f "$PREFIX/bin/llama-perplexity"
          - test -f "$PREFIX/bin/llama-quantize"
          - test -f "$PREFIX/bin/llama-retrieval"
          - test -f "$PREFIX/bin/llama-run"
          - test -f "$PREFIX/bin/llama-save-load-state"
          - test -f "$PREFIX/bin/llama-server"
          - test -f "$PREFIX/bin/llama-simple"
          - test -f "$PREFIX/bin/llama-simple-chat"
          - test -f "$PREFIX/bin/llama-speculative"
          - test -f "$PREFIX/bin/llama-speculative-simple"
          - test -f "$PREFIX/bin/llama-tokenize"
          - test -f "$PREFIX/bin/llama-tts"

          # Check header files
          - test -f "$PREFIX/include/ggml-alloc.h"
          - test -f "$PREFIX/include/ggml-backend.h"
          - test -f "$PREFIX/include/ggml-blas.h"
          - test -f "$PREFIX/include/ggml-cann.h"
          - test -f "$PREFIX/include/ggml-cpp.h"
          - test -f "$PREFIX/include/ggml-cpu.h"
          - test -f "$PREFIX/include/ggml-cuda.h"
          - test -f "$PREFIX/include/ggml-kompute.h"
          - test -f "$PREFIX/include/ggml-metal.h"
          - test -f "$PREFIX/include/ggml-opt.h"
          - test -f "$PREFIX/include/ggml-rpc.h"
          - test -f "$PREFIX/include/ggml-sycl.h"
          - test -f "$PREFIX/include/ggml-vulkan.h"
          - test -f "$PREFIX/include/ggml.h"
          - test -f "$PREFIX/include/gguf.h"
          - test -f "$PREFIX/include/llama-cpp.h"
          - test -f "$PREFIX/include/llama.h"
          - test -f "$PREFIX/include/mtmd-helper.h"
          - test -f "$PREFIX/include/mtmd.h"

          # Check CMake configuration files
          - test -f "$PREFIX/lib/cmake/ggml/ggml-config.cmake"
          - test -f "$PREFIX/lib/cmake/ggml/ggml-version.cmake"
          - test -f "$PREFIX/lib/cmake/llama/llama-config.cmake"
          - test -f "$PREFIX/lib/cmake/llama/llama-version.cmake"

          # Check libraries
          - test -f "$PREFIX/lib/libggml-base.so"
          - test -f "$PREFIX/lib/libggml-cpu.so"
          - test -f "$PREFIX/lib/libggml.so"
          - test -f "$PREFIX/lib/libllama.so"
          - test -f "$PREFIX/lib/libmtmd.so"

          # Check pkg-config file
          - test -f "$PREFIX/lib/pkgconfig/llama.pc"

  - if: (build_platform == target_platform) and (cuda_compiler_version == "None")
    then:
      - script:
          - llama-cli --help
          - llama-server --help
        requirements:
          run:
            - if: cuda_compiler_version != "None"
              then:
                - cuda-version ${{ cuda_compiler_version }}

about:
  homepage: https://github.com/ggml-org/llama.cpp
  repository: https://github.com/ggml-org/llama.cpp
  summary: Port of Facebook's LLaMA model in C/C++
  license: MIT
  license_file: LICENSE

extra:
  recipe-maintainers:
    - jjerphan
    - jonashaag
    - frankier
    - sodre
    - pavelzw
